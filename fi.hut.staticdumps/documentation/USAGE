# Basic usage

Use the bin/dump_sites.php to dump a number of sites, it requires a config file as argument which contains site specific configurations in the following format:

    array
    (
      'url' => 'http://yoursite.com/prefix', // mandatory
      'documentroot' => '/path/to/documentroot', // optional
      'dump_path' => '/tmp/mysite', // mandatory
      'wget_extra_options' => '-k -K', // optional
      'rsync_extra_options' => '--exclude=/midcom-static', // optional
      'protected_htaccess_file' => '/etc/midgard/default.htaccess', // optional
      'protected_htaccess_suffix' => '_midgard', // optional
      'redirect_htaccess_suffix' => '', // optional, remove key alltogether to disable generation of redirection .htaccess files
      'redirect_url_prefix' => 'http://%{HTTP_HOST}', // optional, this is prefixed to redirection urls...
      'username' => 'someuser', // optional
      'password' => 'userpass', // optional
      'pre_dump_script => '', // optional, if returns nonzero exit code, dump will be aborted!, the url and dump path are passed as arguments
      'post_dump_script => '', // optional, the url is passed as argument along with general status indicator exit codes some certain prior operations and dump path are passed as arguments 
    ),

Also if you wish to override some of the more basic settings there are the following config keys and their default values:

    'wget_options' => '-erobots=off -q -m -nH --retry-connrefused',
    'rsync_options' => '-a',
    'http_timeout' => 300, // seconds = 5minutes
    'lockfile_path' => '/var/run',
    'lockfile_prefix' => 'fi_hut_staticdumps_',

The command used to execute `wget` is formed from the wget_options internal default concatenated with the `wget_extra_options` (if defined), ditto for `rsync`. `http_timeout` is used when querying for protected/redirection folder list. If you have multiple nodes sharing the load of dumping a ton of sites `lockfile_path` should point to a shared directory they all can write to, `lockfile_prefix` is configurable for completeness sake.

If using shared directory for lockfiles and dumping from multiple nodes the normal PID checking will not work (since each will check the local processes for validity), and we need to specify a timeout for lockfile in stead.

    'lockfile_use_timeout' => 3600*4, // 4 hours

In the VirtualHost directive of your static apache set the following to handle URLs with GET parameters in them nicely:

    RewriteEngine on
    RewriteCond %{QUERY_STRING} ^.+$
    RewriteRule ^(.*)/(.*?)$ $1/%{QUERY_STRING}_$2? [L]

This will internally convert `file.html?foo` into `foo_file.html` (the dump_sites.php script does the file renames for you in the dump phase)

Alternatively (if you want an offline copy that won't be served by apache, or for whatever other reason) you can add -E (shorthand for `--html-extension`) to `wget_extra_options`

# Dumping as admin ?

Hint: set the following in your `/etc/midgard/midcom.conf` to make life simpler. 

    <?php
    if (   isset($_SERVER['HTTP_USER_AGENT'])
        && strpos('HTTP_USER_AGENT', 'Wget') !== false)
    {
        $GLOBALS['midcom_config_site']['toolbars_enable_centralized'] = false;
    }
    ?>

Surer way to keep your output clean if you need to dump with authenticated user (redirects and protected folders require this), is to make an user that can see what needs to be seen but cannot edit anything.
